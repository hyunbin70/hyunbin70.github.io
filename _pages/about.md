---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am a Ph.D. student at [AMILAB](https://ami.postech.ac.kr/) in Electrial Engineering at [POSTECH](https://postech.ac.kr/eng/), advised by [Tae-Hyun Oh](https://ami.postech.ac.kr/members/tae-hyun-oh). I received my bechelor's degree in Physics from Chung-Ang University.

My research interests are on multi-modal learning, especially within **audio-visual understanding and generation**.

<!-- My research interest includes neural machine translation and computer vision. I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>). -->


# üî• News
- *2024.07*: One paper is accepted to **ECCV 2024**. 
- *2024.06*: Two papers are accepted to **INTERSPEECH 2024**.
- *2024.04*: One paper is accepted to **RA-L 2024**. This will be presented in **IROS 2024** (oral presentation).

# üìù Pre-print (*:equal contribution/co-first author)


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

A Benchmark for Audio-Visual Large Language Models

Kim Sung-Bin\*, **Oh Hyun-Bin\***, JungMok Lee, Arda Senocak, Joon Son Chung, Tae-Hyun Oh

**Project** (to appear)
- We introduce a comprehensive benchmark specifically designed to evaluate the perception and comprehension capabilities of audio-visual LLMs.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Revisiting Learning-based Video Motion Magnification for Real-time Processing](https://arxiv.org/abs/2403.01898)

Hyunwoo Ha\*, **Oh Hyun-Bin\***, Kim Jun-Seong, Kwon Byung-Ki, Kim Sung-Bin, Linh-Tam Tran, Ji-Yun Kim, Sung-Ho Bae, Tae-Hyun Oh

**Project** (to appear)
- We magnify invisible small motions, but in real-time.
- Short version at **CVPRW 2023** ‚ÄòVision-based InduStrial InspectiON‚Äô Workshop.

</div>
</div>


# üìù Publications

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Learning-based Axial Video Motion Magnification](https://arxiv.org/abs/2312.09551)

Kwon Byung-Ki, **Oh Hyun-Bin**, Kim Jun-Seong, Hyunwoo Ha, Tae-Hyun Oh

[**Project**](https://axial-momag.github.io/axial-momag/)
- We magnify invisible small motions, but in user-specified directions.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">INTERSPEECH 2024</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Enhancing Speech-Driven 3D Facial Animation with Audio-Visual Guidance from Lip Reading Expert](https://arxiv.org/abs/2407.01034)

Han EunGi\*, **Oh Hyun-Bin\***, Kim Sung-Bin, Corentin Nivelet Etcheberry, Suekyeong Nam, JangHoon Ju, Tae-Hyun Oh

[**Project**](https://3d-talking-head-avguide.github.io/)
- We generate 3D facial animation speech-synchronized lip movements with audio-visual lip reading expert.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">INTERSPEECH 2024</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MultiTalk: Enhancing 3D Talking Head Generation Across Languages with Multilingual Video Dataset](https://arxiv.org/abs/2406.14272)

Kim Sung-Bin\*, Lee Chae-Yeon\*, Gihun Son\*, **Oh Hyun-Bin**, JangHoon Ju, Suekyeong Nam, Tae-Hyun Oh

[**Project**](https://multi-talk.github.io/)
- We generate accurate 3D talking heads from multilingual speech.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">RA-L 2024</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Uni-DVPS: Unified Model for Depth-Aware Video Panoptic Segmentation](https://ieeexplore.ieee.org/document/10517661)

Kim Ji-Yeon, **Oh Hyun-Bin**, Kwon Byung-Ki, Dahun Kim, Yongjin Kwon, Tae-Hyun Oh

[**Project**](https://jiyeon-k1m.github.io/uni-dvps)
- We present Uni-DVPS, a unified model for Depth-aware Video Panoptic Segmentation (DVPS).
</div>
</div>

<!-- - [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->

# üéñ Honors and Awards
- *2024*: Best Paper Award, KRoC
- *2023*: Best Paper Award, IPIU
- *2021*: Summa Cum Laude, Chung-Ang University
- *2021*: Department Honors Scholarship, Chung-Ang University
- *2018*: Science Scholarship, Suwon Municipal Scholarship Foundation
- *2017*: Department Honors Scholarship, Chung-Ang University

# üìñ Educations
- *2022.03 - Present*, Joint M.S. & Ph.D., Pohang University of Science and Technology (POSTECH), South Korea.
- *2017.03 - 2021.08*, Undergraduate, Chung-Ang University.

# Teaching Experiences
- *2023*, NAVER Boostcamp AI Tech Computer Vision Track (5th, 6th)
- *2022*, Introduction to Machine Learning (EECE454), POSTECH

<!-- # üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

<!-- # üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->